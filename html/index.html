
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
  <title>Cornell ECE 5760 - Bruce in a Box</title>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Language" content="en-us" />
  <link rel="stylesheet" type="text/css" media="all" href="style.css" />

</head>

<body>
  <div id="main">

<!--BANNER#################################################################################################-->
    <!-- <div id="top"><img width="100%" src="images/banner.jpg" /></div>  -->
    <!-- todo -->
    
<!--CONTENT RIGHT COLUMN###################################################################################-->    
    <div id="contentwrapper">
	  <div id="contentcolumn">
	    <div class="innertube">
        
  <!--####INTRODUCTION#####################################################################################-->       
          <h2 id="jump_intro">Introduction</h2>
          
          <p>For my ECE 5760 final project, I have created an augmented reality system that in real time detects the corners of a flat surface and draws an image over the original video feed. Augmented reality is a technology that superimposes a computer generated image on a user’s view of the real world to provide more information. The goal of this project is to celebrate the end of my time as an Electrical and Computer Engineering undergraduate. What better way than to put Bruce Land, one of the best instructors at Cornell, and the Cornell clock tower, onto everything you see. Hence my project, Bruce in a Box.</p>
          
          <p>My system makes use of a variety of digital signal processing and computer vision techniques, resulting in a robust and reliable system. The hardware I utilized was the DE2-115 board by Altera, a Video Camera, and a VGA monitor. All of the functionality of my project is achieved through custom hardware written in Verilog. There are an extensive number of modules, and each one is designed to be as modular and parameterizable as possible.</p>
          
          <p>The user holds up any size of green paper to the video camera. The VGA display screen then displays the scene with an image of either the Cornell Clocktower or Bruce Land superimposed on the green paper. When the user moves or rotates the piece of paper, the image of Bruce or the clocktower rotates, scales, and moves with it.  The final result is an interactive, responsive, and amusing augmented reality system that implements a large range of mathematical, digital signal processing, and computer vision algorithms and concepts.</p>

          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
          
  <!--####HIGH LEVEL DESIGN################################################################################-->
		  <h2 id="jump_high_level">High Level Design</h2>
          
         <p><strong>Rationale and Inspiration:</strong><br />
		 The inspiration for the project came from a Kenyan elections campaign that utilizes augmented reality. The potential voter receives a postcard with a static picture of a flag. When a smartphone camera is pointed at the postcard, the flag animates and plays the national anthem. A link to the video is included in the Appendix.  Having taken a Computer Vision course in the fall and being currently enrolled in a course on Digital Signal Processing, I immediately seized upon this idea. I decided to tailor it to Cornell’s School of Electrical and Computer Engineering, and instead of playing a video on the VGA monitor, display a scalable, 3-D like replica of the Cornell clocktower as well as the instructor of ECE 5760, Bruce Land.
		</p>

		<p><strong>Background Math:</strong><br />
			There are multiple mathematical and computer vision concepts implemented in this project. The big ones are as follows:
		</p>
		  	<ul>
		  	<li>Color Detection based on YCbCr Space</li>
			<li>Median Filtering over space</li>
			<li>Median Filtering over time</li>
			<li>Sobel Edge Detection</li>
			<li>Harris Corner Detection</li>
			<li>Corner Tracking</li>
			<li>Image Transformations, Inverse Warping</li>
			<li>Bubble Sort</li>
			</ul>
		 

		  <p>Color can be represented in a variety of formats. Probably the most common is RGB space, where the red, green, and blue components determine the color of a single pixel. However, RGB color schemes are highly sensitive to changes in lighting. In order to make my system more robust in detecting green, I utilize the YCbCr color space, which is a family of color spaces that separate the luma component, or the light intensity, from the hue component. By using YCbCr space, I am able to always detect the green hue regardless to changes in lighting.</p>

		  <!-- TODO diagram spaces -->

		  <p>Median filtering is exactly what its name implies - a filter takes the median value over a 1-D or 2-D space. The advantage of median filtering is that it throws away outliers while still preserving edges. This differs from a Gaussian smoothing functions, which dilute edges and make all transitions more gradual. A 1-D, n-wide median filter can remove up to floor(n/2) bits of noise.  I perform median filtering over space, in other words on the green pixels detected by my system. This allows me to smooth edges and remove small pixel jags that may be present. I also median filter over time on detected corners. Doing so prevents large jumps and noisy data coming from the corner detection module. The detected corners of the green square move smoothly without a significant delay. Each detected green pixel is also median filtered over time. This ensures that only pixels that are consistently green are factored into corner calculations. A diagram of how 1-D and 2-D filtering operate on a signal is included below.</p>

		  <!-- TODO median filtering -->

		  <p> Sobel Edge Detection and Harris Corner Detection are two Computer Vision Algorithms used to extract features from an image. While my final system does not utilize Harris Corner Detection, I fully implemented and tested this algorithm. The algorithm is as follows:</p>
		<ol>
			<li>Use an nxn Sobel filter to get the x-gradient and y-gradient from the image</li>
			<li>For a window of size wxw, calculate the following matrix (insert Ix, Iy, IxIy):
					<!-- TODO Ix Iy IxIy matrics -->
			Optionally weight the values of Ix and Iy by the distance of that pixel from the center pixel of the window</li>
			<li>Calculate the determinant and trace of the matrix, and use that to calculate c, the corner strength feature formula corner strength</li>
			<li>For all values of c above a certain threshold, mark that pixel as a corner</li>
		</ol>

		<p>Tracking the corners of a square in 3-D space falls underneath the larger problem of resolving motion ambiguities, a difficult task. The mathematical approach to resolving motion ambiguity is to use coupled Markov random fields that estimate uncertainty on a local area. These estimates are then used to determine which direction the object rotated. However, this is a rather complicated algorithm involving a lot of divisions, a difficult task on an FPGA. So I opted to create my own algorithm to track the corners of the rotating square. This algorithm will be discussed in detail in the Hardware Design Section.</p>
		<p>Image transformations are operations that can translate, rotate, skew, scale, shear, or project an image in 2D space. Normally the transformation is represented in one or several matrices, and these matrices are convolved over the entire image.  There are two ways to get from a pixel’s original location in an image to its new location. The first method is forward warping. In this method, every point in the original image is transformed and “sent” to its new location.   However, this mode of warping can result in holes and splattering. The better approach is to perform inverse warping.  This algorithm goes through every pixel in the new, transformed image, undoes the transformation, and figures out which original pixel to grab.  If the original pixel happens to fall between two pixels, simply interpolate the source image. My system implements inverse warping on translation, rotation, and scale operations in order to redraw the superimposed image on to the VGA display.</p>

		<!-- TODO inverse and forward warping -->
		<p>Bubble sort is the simplest sorting algorithm. It repeatedly steps through a list of items to be sorted and swaps items if they are out of order. The algorithm continues stepping through the list until all entries are sorted. Bubble sort, while simple, is inefficient, achieving at worst O(n^2) performance. I have implemented Bubble Sort as a Verilog task that can be called repeatedly by different portions of a module.</p>

		  <p><strong>Logical Structure:</strong><br />
         The DE2-115 board receives a video feed in NTSC standard video format from a digital camcorder.  The DE2-115 saves the incoming pixel information to an on-board SDRAM and later converts the video feed into YCbCr and RGB formats. Then, a VGA controller takes the RGB information and generates the appropriate VGA control signals.  In order to manipulate the data to the VGA screen, I intercept the pixel color data and VGA control signals coming out of the VGA controller and put them through a 20 VGA clock delay buffer. At the same time, my system utilizes the color information and several VGA control signals to come up with the new image to be superimposed on the original video feed. Thus, at the end of 20 clock cycles, mux’s decide whether the original VGA pixels from the video input are used or whether pixels generated by my system during that 20 cycle delay should be displayed on the VGA screen. By using this buffering system, I am able to manipulate the image on the VGA screen in real time. There is technically a slight lag between the real world and what’s displayed on the VGA screen, but it is so small that it is not noticeable by the user. A logical overview of the system is included below:
		</p>

          <a href="images/logical.jpg"><img width="75%" src="images/logical.jpg" /></a>

		  <p><strong>Hardware/Software Tradeoffs:</strong><br />
		  There is no software in this design; all functionality is implemented using full custom Verilog hardware. I chose to stick with full custom hardware because I have finer grained control over resource utilization. Additionally, had I chosen to use a Nios II system, there the potential for severe bus contention on the Avalon switch fabric since so many modules are attempting to communicate with each other.
		</p>
		  
		  <p>In hardware, there are several tradeoffs between speed, space, and accuracy. When median filtering over time, i.e. over VGA frames, using more frames leads to a cleaner edge. However, to store more history, more SRAM must be used, and there is a longer lag effect on the screen. To address this tradeoff, I allow the user to choose anywhere from 0 - 4 frames for the time averaging threshold.</p>
		  
		  <p>When median filtering over time on the corners, the longer the filter the more outliers you can remove. However, there is limited storage in SRAM, and the more history you keep, the more SRAM you use. I found that median filtering over 11 frames created smooth corner tracking without hogging a lot of SRAM space.
		</p>
		  
		  <p>Finally, when median filtering over space I use a 5x5 median filter. Again, a larger filter removes more outliers, but also requires buffering more of the incoming VGA pixels.  I found that 5x5 median filtering removes a lot of green pixel noise while keeping the RAM utilization reasonable.</p>
		
			<p>When deciding which Altera Board to use, the DE2 or the DE2-115, I chose to the use the DE2-115 because it has more dedicated multipliers and SRAM. The downside is that throughout the majority of this course, we have used the standard DE2 board. There was some overhead in getting the Altera example project up and running on the DE2-115 board, but this cost was far outweighed by the benefits of increased computational power and memory.</p>

		  <p><strong>Relationship of design to standards:</strong><br />
		  My design requires the video signal to be in NTSC video standard form. The data is output as a VGA signal. Thus, television and computer display standards are at play. These standards are implemented using IP cores from Altera that are included in the DE2_115_TV example.</p>
		  <p>There are numerous patents held by companies regarding augmented reality systems. However, I did not reference any of these patents nor do I plan on patenting this invention, so I will spare you and myself a detailed patent search.</p>

          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####HARDWARE DESIGN##################################################################################-->
          <h2 id="jump_hardware">Hardware Design</h2>
          
          <p><strong>Overview</strong><br />
       		The hardware in this design must accomplish a series of tasks in order to make this project work.  The steps are as follows:</p> 

		  <p>This module, while originally designed in registers, was optimized to M4K blocks by the compiler.  This makes its operation slightly slower, but otherwise we would not have enough registers for the entirety of our project.</p>
          
          <a href="images/Buffer.jpg"><img width="50%" src="images/Buffer.jpg" /></a>

		  <p><strong>Intensity Calculator:</strong><br />
		  The intensity calculator module is a simple hardware implementation of a weighted averaging between the different color channels of the pixel. The red, green, and blue values of the pixel in question are given to the intensity calculator during each cycle while the VGA is not blanking. We chose to weight the intensity of a pixel as 25% red value, 50% green, and 25% blue. We chose green to be greater than red and blue because the human eye is most sensitive to this color. The coefficients are chosen as powers of 2 so we can use shift and add operations instead of costly multiplications. We let the compiler optimize the addition and shift operations as necessary and register the result each cycle, which is then outputted to the edge detectors. We instantiated 9 intensity calculator modules in our final design to account for the block of 9 pixels that the edge detection algorithm requires.</p>

		  <p><strong>Edge Detectors:</strong><br />
		  There are two distinct modules for edge detection, one that does the vertical edges, and one that does the horizontal edges.  The only difference between the two is which convolution filter it uses (described in Background Math), the rest of the execution is exactly the same.  The input grid of 9 pixels comes from the intensity calculator.  The module then convolves this grid of pixels with the filter, and does the necessary additions and subtractions.  This sum is then checked against a threshold.  We calculate two versions of the sum, since there is a possibility it will be negative.  We calculate the original sum and the negative of it, which guarantees that one of the two will be positive.  Then we check which one is positive, and check that value against the threshold.  This avoids some possibly disastrous complications, which include difficulty with sign on comparisons, or the need for two thresholds, 1 positive and 1 negative.</p>
          
          <p>The threshold for this module is variable, and can be set using the switches and a push button to register the new threshold value.  We originally had the threshold variable at 16 bits, but this was causing timing errors that resulted in smearing on the screen.  We reduced the threshold to only be a 10 bit number, which removed the timing issues.  The module outputs a 1 if the pixel is an edge, and a 0 if the pixel is not an edge.</p>

		  <p><strong>Time Averager:</strong><br />
		  We decided to average the pixel data both in time and space to reduce the noticeable flicker caused by color simplification. The module uses the on-board SRAM to store a history of past values of blocks of pixels to perform a weighted average on along with new incoming pixel data. The module is split into two components: an averager module that performs a combinational weighted average, and the hardware associated with SRAM communication.</p>

		  <p>The averager module takes in the color information of the current pixel, the history for that pixel, and outputs the new value for the pixel to be saved in SRAM. We designed the averager to keep a running average of the last 4 frames worth of pixel data for each pixel in the raster. This means we want to add ¾ of the old value to ¼ of the new value to get the updated average. We chose to average 4 frames because it is a power of 2 and can be calculated using shift and add operations, thus avoiding costly multiplications. Instead of trying to calculate ¾ of the old value directly, we use (1 - ¼) times the old value. The final equation for the new average is:</p>
		  <center>newAvg = oldAvg - oldAvg>>2 + newColor>>2.</center>
		  <p>The averager module assumes that each color channel will have 5 bits of precision. This is determined by the word size of the SRAM, which is 16 bits. We have three 5 bit colors, which is 15 bits, and 1 unused bit that is stored in the SRAM. All three colors are averaged at the same time within the averager, and the value is outputted without registering.</p>
		  
		  <p>The time averager module is responsible for communicating with the SRAM and feeding its data to and from the averager module. To implement a 2x2 spatial averager, first we needed to change the addressing of the pixels by removing the least significant bit of the X and Y coordinates. This makes 4 adjacent pixels all reference the same location in SRAM. This reduces the total required SRAM and allows us to use a full word for pixel data instead of sharing 2 pixels per 16 bit word of SRAM as we have done in the past.</p>
		  
		  <p>The SRAM takes a cycle to read and another to write, which is a problem if a new pixel is required each cycle. Luckily we are averaging 2 pixels at a time and adjacent horizontal pixels are requested one after another. This means we only need the history once for both the pixels together instead of both individually, which gives us 2 cycles per 2 pixel horizontal pair. We take advantage of this by reading the history for the two pixels in the first cycle and write the updated history for the two pixels in the second cycle. The history gets updated with both new pixel values for that 2 pixel block by using an intermediate history value that is stored in a register and not in SRAM.</p>
		  
		  <p>The current address requested is always sent to the SRAM address port with the LSB of X and Y coordinates removed. This makes the address the same during the first and second cycle of a pixel pair. The write enable for the SRAM is only set high in the second cycle, which is determined by the LSB of the X coordinate. There is a single data port to the SRAM that must be floating during the first cycle for reading, and set to the updated history value from the averager during the second cycle. The write enable to the SRAM can only be set high when the VGA controller is not “Blanking” because we only want to update the pixel history with valid data being requested by the VGA controller.</p>
		  
		  <p>The input to the averager module’s “old” port during the first cycle is the history data read from the SRAM. The input to the current colors are always the current color channels coming from the SDRAM requested by the VGA controller. The output of the averager is registered after the first cycle and used as the “old” input to the averager in the second cycle. The output during the second cycle is sent to the SRAM. This system uses both current adjacent pixel colors to update the average, while only needing to update the SRAM once. The output of the averager is also set as the output of the timeAverager module for each cycle. This is in a 5-5-5 RGB format.</p>

		  <p><strong>Muxing / Pipelining:</strong><br />
		  Because we need to operate in real-time, we separate the computation into 2 pipelines for computation.  One pipeline is responsible for calculating the edges, while the other does the time averaging and color reduction.  For the edge detection pipe, the first stage is to run through the 3-line buffer.  For the color reduction pipe, the first stage is pipelining logic for the VGA signal inputs to the Time Averager.</p>

		  <p>For the color reduction pipeline, the stage 2 operation is the Time Averager.  For the edge detection pipeline, stage 2 is the intensity calculator.  Stage 3 is then the edge detectors, with the output of the two detectors being combined in an OR gate.  For the other pipeline, stage 3 is the 3-line buffer.  This is important because we must ensure that both pipes have the same amount of delay, so that the edges line up with the color image.  Stage 4 is where the two pipes are combined.  The user input masking logic is also here.  The muxing is done in three stages, first we choose between color or grayscale.  Next we choose the number of significant bits to use.  Finally we choose between the color/grayscale pixel, or a black pixel (edge).</p>
          
          <p><strong>VGA Controller Modifications:</strong><br />
		  The VGA controller that Terasic kindly provides is defaulted to send a pixel request and receive the data from SRAM all in one cycle. Without any additional hardware this is a tight timing path to meet. In addition to this, we have to insert several pipeline stages and hardware modules to do our edge detection and color simplification. To make sure the VGA controller is receiving the data it thinks it is, we must send a request for the data a certain number of cycles early to allow for the delay through all the extra pipeline stages that we have added. We added 2 lines (minus 1 pixel) of pixel delay for the shift registers, 1 cycle for the intensity calculation, 1 cycle for the edge detection, and 1 cycle for the MUXing, which totals 2 vertical lines and 2 horizontal pixels worth of delay.</p>
		  
		  <p>Within the VGA Controller we added H_DLY and V_DLY parameters to add in these pixel delays. They cause the controller to start sending its stream of pixel requests before the end of the HSYNC by the delay amount specified. This means we actually start requesting pixels during the horizontal sync so the data arrives by the time the rest of the controller expects it.</p>
          
		  <p><strong>RGB to HSL and Back:</strong><br />
		  In order to do hue simplification to cartoonify our image, we had to first map our RGB input into HSL space. This proved much easier in theory than in practice. We found that there is basically one way to do it, although one can change the ranges used to represent values to adjust the algorithm to different environments. Our algorithm is based on the formulas listed <a href="http://130.113.54.154/~monger/hsl-rgb.html">here</a>, which are taken from Fundamentals of Interactive Computer Graphics by Foley and van Dam (c 1982, Addison-Wesley), Chapter 17. These for the most part keep values in the range of 0 to 1. However, at some points in the formula temporary values exceed 1. Also, it maps hue to 0 to 6, since it is important for hue to be divisible by 3. On top of the weird numbers, converting RGB to HSL involves several divides, and converting back involves several multiplies. The formulas given also use negative temporary values sometimes. This makes for a difficult choice of hardware implementation.</p>

          <p>Difficult yes, impossible, no. We settled on doing all the math in 1.8 fixed point with occasional flags for negatives as needed. 1.8 fixed point meant we had 8 bits for the range 0 to 1 where most of the action happens and had a carry bit for where it was needed for temporary values. 1.8 fixed point also fits nicely into the 9 bit multipliers. This solves all the issues except hue, with the formulas going up to 6 in value and mandating divisibility by 3. To get around this, we stored our hue value as 1.8 fixed point with a maximum of 0.11000000 (3/4ths, or 192 as an integer). By sacrificing 1/4th of our range, we gained the divisibility by 3 needed for mapping 3 colors to one hue value and then the 1 hue back to 3 colors. See RGBtoHSL.v and HSLtoRGB.v for verilog source code.</p>
          
          <p>1.8 unsigned fixed point multiplication means one keeps bits 16:8 out of the 17:0 hardware “*” result. For division, first shift the quotient left 8. Then divide. Then keep bits 8:0 out of the 17:0 hardware “/” result. As a sanity check, this means dividing by 1.00000000 will give the same result - hardware division is integer division so dividing by 1.00000000 means shifting right 8.</p>
          
          <p>See ufpMult.v and ufpDiv.v for verilog source code.</p>

		  <p><strong>Lookup Table for Hue Simplification:</strong><br />
          Unfortunately, even the correctly implemented RGB to HSL conversion formulas left some ugly artifacts due to the loss of precision and finicky edge cases. However, since we had RGB input and needed an RGB result, there was no explicit need to deal with HSL in hardware. Therefore, we created a hue simplification module that just used a lookup table that mapped RGB input to RGB output. The table sized we settled on was 3 bits per color, so total size was 2^9 or 512 entries. We tried 4 bits per color, but 2^12 meant 4096 entries which was too slow in hardware. See the software section for how we populated the table entries. The verilog code is in RGB_HueSimplifier.v</p>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####SOFTWARE DESIGN##################################################################################-->
          <h2 id="jump_software">Software Design</h2>
          
          <p><strong>Color Simplification Test Script (MATLAB): RGB, HSL, Edge Detect:</strong><br />
          In order to quickly see the results of various color simplification schemes, we wrote a MATLAB function to simulate what our hardware would do. The script takes in an image, simplifies the colors, and outputs two images: one with edges highlighted and one without. Since the script ran in seconds and the simplification scheme was easy to adjust, this let us test out a lot more schemes than would’ve been possible by repeatedly changing them in hardware and waiting for the Verilog to compile.</p>
          
          <p>For color simplification in HSL space, we first used a 3rd party script to convert RGB values to HSL values. We then rounded each of the hue, saturation, and lightness values into some determined set of values. For example, if we specified 32 hues, hue was rounded to the nearest 1/31nd on a 0 to 1 scale - 31 was used since both 0 and 1 show up when rounding. We then used a 3rd party script to convert the HSL back to RGB.</p>
          
          <p>For color simplification in RGB space, we simulated the effect of simply dropping bits on the FPGA by using floor() to round values. Each of the red, green, and blue values was multiplied by 2^n, floor()’d, then divided by 2^n to effectively keep only the most significant n bits of the fixed point value that is used on the FPGA.</p>
          
          <p>Our edge detection in software was algorithmically identical to the edge detection used in hardware. Both use a Sobel edge detection filter. The major difference was in software we ran it by looping over a 3 pixel wide band of a static image, where as in hardware it handed a 3 pixel wide stream. The software code currently allows for the output of edges to be half as dark when an edge is only detected in one direction.</p>
          
          <p>See <a href="index.html#jump_appx_photos">Appendix A</a> for photos</p>
          
          <p><strong>HSL Simplification Lookup Table Generator (MATLAB):</strong><br />
          During development we eventually decided using a lookup table for hue based simplification was better than doing the HSL to RGB conversion and back in hardware. Since we had the conversion working both ways in MATLAB, we utilized that and made a MATLAB script that generates a lookup table mapping RGB input values to RGB output values that have been simplified by rounding in HSL space. The script generates a list of all possible inputs (based on using 3 bits per color), converts them all into HSL values, rounds them according to some scheme, then converts them back to RGB and lists the output next to each respective input. We decided on a 9 bit lookup table because a 12 bit table was too slow and caused massive horizontal smearing in the output. However, this means our HSL simplification process is not only simplifying in hue space, it is simplifying in RGB space since it only has 3 bits per color of input and output for the table.</p>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####DEVELOPMENT PROCESS##############################################################################-->
          <h2 id="jump_development">Development Process</h2>
          <p><strong>General Hardware Design:</strong><br />
          We started the project design with a few brainstorming sessions, at the end of which we had a block level diagram of what we thought the system would look like.  Our first step in the design was to get the 3-line buffer in, without any other modifications.  This involved tweaking the VGA controller to send addresses earlier than it needed them back, since the data was received significantly later.</p>
          
          <p>Once we had the buffer in, and outputting to the screen without any noticeable defects, we began to work on the edge detection.  We started by designing the filters, and setting up the system to display an all black background, with only white pixels where there were edges.  We used this video feedback to tune our threshold for the edge detection.  We wanted edges to be thick and this method allowed us to see the edges change as we changed the threshold.</p>
          
          <p>After implementing the edge detection scheme, we moved to working on the color reduction aspect of the lab.  This started by just reducing the number of bits we use in the color.  This produced a decent output, but we thought that by converting to hues we might be able to do better.  Kerran took the hue conversion aspect and ran with it, and the debugging for this specific aspect can be seen below.  During this time, we also realized that we had a major problem with flicker, which we chose to address by using time averaging.  We designed a time averager and dropped it in, and we also decided to stick with a simple RGB reduction for our output scheme.</p>
          
          <p>Once we had the system pretty much finalized, we decided to put in some extra features that would help with the demo.  We put in variable edge detection, which can be set using the switches and then locked in using a push button.  We also made it variable for the number of bits we use for each color.  There are a maximum of 5 bits for each color, which each have 5 switches, which can be set active or inactive and locked in using a push button.  We allow for viewing in grayscale, by toggling switch 0.  Finally we can adjust the brightness by toggling switch 1, which controls wheter the inactive bits are set to zero or one.</p>
          
          <p><strong>Debugging HSL Conversion</strong><br />
          The most useful thing we found for debugging the HSL conversion process was to point the camera at a color wheel or linear color spectrum. These spatially organized colors meant we could look for breaks in the spatial patterns in our output that indicate edge case failures, precision issues, or bugs. Our standard output was the final RGB result, but we found it very useful to take advantage of the ability to put things besides red, green, and blue on the three channels. The first channel, which always appeared as a red value on the monitor, could be either the actual red output, nothing, or the hue value output. The second was green, nothing, or the saturation. The third was blue, nothing, or lightness. This allowed us to debug each of the three aspects of HSL conversion independently.</p>
          
          <p>The horizontal hue spectrum below, was great for hue testing, and appeared shaded from dark to bright red left to right when our only enabled output was hue on the red channel. We found this image on a project page from a previous year <a href="http://instruct1.cit.cornell.edu/courses/ece576/FinalProjects/f2007/tjs49aw259/index.html">here</a>.</p>
          
          <a href="images/spectrum.png"><img width="75%" src="images/spectrum.png" /></a>
          
          <p>The hue-saturation color wheel below was good for testing hue and saturation. When outputting only saturation (green channel), the wheel appeared on our output as dark in the center with bright green edges. There were a few regions where the saturation maxed out prior to the edge of the circle, likely due to camera and screen signal processing as well as hardware mathematical imprecision. The hue did a nice job of wrapping around the circle going from dark to light, with the split down the center of the red region. As with saturation though, there were a few blobs on the edges due to imprecision and all the intermediate processing.  We found this picture <a href="http://commons.wikimedia.org/wiki/File:Color_circle_(hue-sat).png">here</a>.</p>
          
          <a href="images/colorwheel.png"><img width="50%" src="images/colorwheel.png" /></a>
          
		  <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####RESULTS##########################################################################################-->
          <h2 id="jump_results">Results</h2>
          <p>For our final result, we had a complete cartoonifier system that ran in real time.  We could variably set the edges and the bit simplification for each color.  We could display in both color and grayscale.  We accomplished the goals that we set out to accomplish, and the video output looked like what we expected.</p>
          
          <p>There is a tiny bit of delay in the system, that is only noticeable if you watch a moving mouth and directly compare it to the rendered image on the screen.  There is a minimal amount of flickering of colors, but not enough to distract from the overall execution of the system.  In general, it works well, and it does what we expected of it.</p>
          
          <p>Below are two sample images of our output, one in grayscale and one in color.  The color image shows the rendered image next to the real image.  There are additional photos in the <a href="index.html#jump_appx_photos">appendix</a>.</p>
          
          <a href="images/Group_gray.jpg"><img width="50%" src="images/Group_gray.jpg" /></a>
          <br />
          <a href="images/Tom_beside_himself.jpg"><img width="50%" src="images/Tom_beside_himself.jpg" /></a>
          
		  <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####CONCLUSIONS######################################################################################-->
          <h2 id="jump_conclusions">Conclusions</h2>
          <p>Overall we were very satisfied with the results of our project. We implemented a real-time cartoonifier that used an edge detection algorithm, time/space averaging, and selectable color simplification. The images we can produce are very convincing and in many cases look like they were sketched by an artist. We were able to accomplish all of this while keeping the hardware fast enough to keep up with the constantly updating stream of video data from the camcorder. We were able to conform to the NTSC video standard both while reading data from the camera and while outputting data to the monitor.</p>
          
          <p>However, we were not able to produce ideally cartoonized images for several reasons. There was a noticeable amount of flickering in our video due to the color simplification scheme. We were able to reduce this somewhat by averaging pixels together both in time and space. We would have liked the flickering to be minimized further. Also, we initially wanted fewer more vibrant colors so that everything within an edge boundary would be the same color. We tried to accomplish this using HSL simpflication but ran into problems. Mostly the colors that came out looked strangely different than what they should be using this scheme. We would have liked to use fewer, more vibrant colors, but non-ideal lighting and “boring” colors make this difficult.</p>
          
          <p>This was a very fun and interesting project that challenged us in video processing, real-time calculation, and embedded system design. This design could be produced, but a large portion of it is based off of the Terasic example code that came with the DE2 board.</p>
          
		  <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####APPENDIX A: PHOTOS########################################################################-->
          <h2 id="jump_appx_photos">Appendix A: Photos/Videos</h2>
          
          <p>Final output images</p>
          <ul><li><a href="images/Group_gray.jpg">Group Grayscale</a></li>
          <li><a href="images/Jeff_gray.jpg">Jeff Grayscale</a></li>
          <li><a href="images/Kerran_gray.jpg">Kerran Grayscale</a></li>
          <li><a href="images/Tom_gray.jpg">Tom Grayscale</a></li>
          <li><a href="images/Kerran_Tom_Jeff.jpg">Group color w/ setup</a></li>
          <li><a href="images/Jeff_color.jpg">Jeff color w/ setup</a></li>
          <li><a href="images/Tom_beside_himself.jpg">Tom color</a></li></ul>
          
          <p>HSL simulation images</p>
          <ul><li><a href="images/hsl/Colorful_People.jpg">Original Image</a></li>
          <li><a href="images/hsl/hsl_16_3_3_noEdges.jpg">HSL, 16 Hues, 3 Saturation Levels, 3 Lightness Levels, no Edges</a></li>
          <li><a href="images/hsl/hsl_16_3_3.jpg">HSL, 16 Hues, 3 Saturation Levels, 3 Lightness Levels, with Edges</a></li>
          <li><a href="images/hsl/hsl_32_4_4_noEdges.jpg">HSL, 32 Hues, 4 Saturation Levels, 4 Lightness Levels, no Edges</a></li>
          <li><a href="images/hsl/hsl_32_4_4.jpg">HSL, 32 Hues, 4 Saturation Levels, 4 Lightness Levels, with Edges</a></li>
          <li><a href="images/hsl/rgb_2_2_2_noEdges.jpg">RGB, 2 bits of each color, no edges</a></li>
          <li><a href="images/hsl/rgb_2_2_2.jpg">RGB, 2 bits of each color, with edges</a></li>
          <li><a href="images/hsl/rgb_3_3_3_noEdges.jpg">RGB, 3 bits of each color, no edges</a></li>
          <li><a href="images/hsl/rgb_3_3_3.jpg">RGB, 3 bits of each color, with edges</a></li>
          <li><a href="images/hsl/rgb_4_4_4_noEdges.jpg">RGB, 4 bits of each color, no edges</a></li>
          <li><a href="images/hsl/rgb_4_4_4.jpg">RGB, 4 bits of each color, with edges</a></li></ul>
          
          <p>Videos</p>
          <ul><li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_1_tom.MOV">Tom - Project Description</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_2_jeff_line.MOV">Jeff - Edges Only</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_3_bruce_lines.MOV">Bruce - Edges Only</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_4_edge_off.MOV">Group - Grayscale, Edges Off</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_4_edge_on.MOV">Group - Grayscale, Edges On</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2010/kaf42_jay29_teg25/Demo_5_brl4_gray.MOV">Bruce - Grayscale</a></li></ul>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
 
  <!--####APPENDIX B: OUR CODE######################################################################-->
          <h2 id="jump_appx_code">Appendix B: Our Code</h2>
          
          <p>Final system code</p>
          <ul><li><a href="code/averager.v">averager.v</a></li>
          <li><a href="code/buffer3.v">buffer3.v</a></li>
          <li><a href="code/DE2_TV.v">DE2_TV.v</a></li>
          <li><a href="code/edgedetectH.v">edgedetectH.v</a></li>
          <li><a href="code/edgedetectV.v">edgedetectV.v</a></li>
          <li><a href="code/intensityCalc.v">intensityCalc.v</a></li>
          <li><a href="code/newPress.v">newPress.v</a></li>
          <li><a href="code/TimeAverager.v">TimeAverager.v</a></li>
          <li><a href="code/VGA_Ctrl.v">VGA_Ctrl.v</a></li>
          <li><a href="code/DE2_TV.sof">.sof program file</a></li></ul>
          
          <p>HSL code</p>
          <ul><li><a href="hsl/HSLtoRGB.v">HSLtoRGB.v</a></li>
          <li><a href="hsl/RGB_HueSimplifier.v">RGB_HueSimplifier.v</a></li>
          <li><a href="hsl/RGBtoHSL.v">RGBtoHSL.v</a></li>
          <li><a href="hsl/ufpDiv.v">ufpDiv.v</a></li>
          <li><a href="hsl/ufpMult.v">ufpMult.v</a></li>
          <li><a href="hsl/ColorSimplify.m">ColorSimplify.m</a></li>
          <li><a href="hsl/hsl2rgb.m">hsl2rgb.m</a></li>
          <li><a href="hsl/rgb_hslSimplify_rgb.m">rgb_hslSimplify_rgb.m</a></li>
          <li><a href="hsl/rgb2hsl.m">rgb2hsl.m</a></li></ul>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
           
  <!--####APPENDIX C: COST DETAILS##################################################################-->          
          <h2 id="jump_appx_cost">Appendix C: Cost Details</h2><br />
          
          <p>The DE2 Development board we used was generously donated by Altera, and the video camcorder was lent to us by Professor Bruce Land. Any other resources used, such as VGA cables and monitors were provided in the lab.</p>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
          
  <!--####APPENDIX D: TASK BREAKDOWN#######################################################################-->
          <h2 id="jump_appx_tasks">Appendix D: Task Breakdown</h2><br />
          <table cellspacing="0" cellpadding="0">
            <col width="260" />
            <col width="83" />
            <tr>
              <td width="260"><b>Task</b></td>
              <td width="83"><b>Contributor</b></td>
            </tr>
            <tr>
              <td>3-line Buffer</td>
              <td>Tom</td>
            </tr>
            <tr>
              <td>Intensity Calculator</td>
              <td>Jeff</td>
            </tr>
            <tr>
              <td>Edge Detectors</td>
              <td>Tom</td>
            </tr>
            <tr>
              <td>Time Averager</td>
              <td>Jeff</td>
            </tr>
            <tr>
              <td>Muxing</td>
              <td>Tom</td>
            </tr>
            <tr>
              <td>Pipelining</td>
              <td>Jeff</td>
            </tr>
            <tr>
              <td>MATLAB code</td>
              <td>Kerran</td>
            </tr>
            <tr>
              <td>HSL code</td>
              <td>Kerran</td>
            </tr>
            <tr>
              <td>Debugging</td>
              <td>All</td>
            </tr>
          </table>
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
          
  <!--####References#######################################################################################-->          
          <h2 id="jump_references">References</h2>
          
          <p>We referenced much of Bruce Land's lecture material, especially with regards to edge detection schemes. We also used the TV Decoder module provided by Terasic, from a CD which came with the DE2 Development board. We referenced a few websites for general background information, including our HSL conversion algorithm, which are listed below.</p>
          
          <h3>Background Sites</h3>
          <ul><li><a href="http://130.113.54.154/~monger/hsl-rgb.html">HSL Algorithm</a></li>
          <li><a href="http://instruct1.cit.cornell.edu/Courses/ece576/FinalProjects/f2009/jsd46_pag42/jsd46_pag42/index.html">Previous Green Screen Project</a></li>
          <li><a href="http://www.altera.com/">Altera</a></li>
          <li><a href="http://www.terasic.com">Terasic</a></li>
          <li><a href="http://www.nbb.cornell.edu/neurobio/land/">Bruce Land</li></ul>
          
          <h5><a href="index.html#top">[top]</a></h5>
          <hr />
          
  <!--####SPECIAL THANKS###################################################################################-->
		  <h2 id="jump_thanks">Special Thanks</h2>
          
          <p>We would like to thank Altera for donating the DE2 FPGA boards to Cornell University for us to use. We would also like to thank Terasic for their example code, specifically the VGA Controller and Video Decoder code. Last but certainly not least, we would like to thank our friend and instructor, Bruce Land. Bruce helped in countless ways during the planning and implementation of this project and throughout the entire course on assignments leading up to this one.</p>
          
        </div> <!--end innertube-->
	  </div> <!--end content column-->
	</div> <!--end content wrapper-->

<!--LEFT NAVIGATION COLUMN#################################################################################-->
	<div id="leftcolumn">
	  <div class="innertube">
        <h3><a href="index.html#jump_intro">Introduction</a></h3>
        <h3><a href="index.html#jump_high_level">High Level Design</a></h3>
        <h3><a href="index.html#jump_hardware">Hardware Design</a></h3>
        <h3><a href="index.html#jump_software">Software Design</a></h3>
        <h3><a href="index.html#jump_development">Development Process</a></h3>
        <h3><a href="index.html#jump_results">Results</a></h3>
        <h3><a href="index.html#jump_conclusions">Conclusions</a></h3>
        <h3>Appendices</h3>
          <ul>
        	<li><a href="index.html#jump_appx_photos">A: Photos/Videos</a></li>
            <li><a href="index.html#jump_appx_code">B: Our Code</a></li>
            <li><a href="index.html#jump_appx_cost">C: Cost Details</a></li>
            <li><a href="index.html#jump_appx_tasks">D: Task Breakdown</a></li>   
          </ul>
        <h3><a href="index.html#jump_references">References</a></h3>
        <h3><a href="index.html#jump_thanks">Special Thanks</a></h3>
        
        
      </div> <!-- end innertube-->
	</div> <!-- end leftcolumn-->
    
<!--FOOTER#################################################################################################-->
	<div id="footer">&copy;Spring 2014 Julie Wang</div>

  </div> <!-- end main-->
</body>
</html>
